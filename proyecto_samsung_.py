# -*- coding: utf-8 -*-
"""Proyecto_SAMSUNG_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11AZEUWmm70iRx7VLNedikhNNPI2Xxpn9

# Alejandro Flores Jacobo
# Samsung Innovation Campus
# Proyecto: Predicctor de calidad de Aire en Mexico
# 04 de abril del 2025

#Exploración de datos
"""

import pandas as pd

# Ruta al archivo csv
file_path = '/content/drive/MyDrive/SAMSUNG_IA_2/stations_daily.csv'

# Leer el archivo csv
df = pd.read_csv(file_path)
df.info()

# Convertir station_id a tipo de dato categórico
df['station_id'] = df['station_id'].astype('category')

# Convertir la columna 'datetime' a tipo de dato datetime
df['datetime'] = pd.to_datetime(df['datetime'])

# Verificar los cambios
df.info()

df.describe()

df.head()

df.tail()

# Odenar el df por categoria y fecha de forma ascendente
df_sorted = df.sort_values(by=['station_id', 'datetime'], ascending=[True, True])

df_sorted.head()

df_sorted.tail()

# Verificar duplicados
duplicates = df_sorted.duplicated(subset=['datetime', 'station_id'], keep=False)

df_sorted[duplicates]

# Obtener el rango mínimo y máximo de fechas en el DataFrame completo
min_date = df_sorted['datetime'].min()
max_date = df_sorted['datetime'].max()

# Crear un rango completo de fechas con frecuencia de 1 dia
full_date_range = pd.date_range(start=min_date, end=max_date, freq='d')

full_date_range

# Obtener la lista única de station_id
unique_stations = df['station_id'].unique()

# Crear un DataFrame con todas las combinaciones de station_id y datetime
full_index = pd.MultiIndex.from_product(
    [ full_date_range, unique_stations],
    names=['datetime', 'station_id']
)

# Crear un DataFrame vacío con el índice completo
df_complete = pd.DataFrame(index=full_index).reset_index()

unique_stations

df_complete.info()

df_complete

# Odenar el df_complete por categoria y fecha de forma ascendente
df_complete = df_complete.sort_values(by=['station_id', 'datetime'], ascending=[True, True])

df_complete

# Combinar el DataFrame completo con el DataFrame ordenado
df_filled = pd.merge(df_complete, df_sorted, on=['datetime', 'station_id'], how='left')

df_filled

# Exportar df_filled a un archivo csv
df_filled.to_csv('/content/drive/MyDrive/SAMSUNG_IA_2/df_filled.csv', index=False)

"""------"""

import pandas as pd

# Leer df_filled desde el archivo csv
df_filled = pd.read_csv('/content/drive/MyDrive/SAMSUNG_IA_2/df_filled.csv')

# Convertir la columna 'datetime' a tipo de dato datetime
df_filled['datetime'] = pd.to_datetime(df_filled['datetime'])

# Convertir station_id a tipo de dato categórico
df_filled['station_id'] = df_filled['station_id'].astype('category')

# Verificar el DataFrame cargado
df_filled.info()

df_filled.columns[7:]

# Filtrar características no necesarias en base al artículo fuente: "Colocar su fuente"
_df_filled = df_filled.drop(columns=df_filled.columns[7:])

# Filtrar estaciones para quedarse solo con las seleccionadas: "Colocar su fuente"
selected_stations = [102, 103, 106, 109, 125, 126, 128, 139, 140, 141, 142, 143, 256, 260, 266, 268, 271]
_df_filled = _df_filled[_df_filled["station_id"].isin(selected_stations)]

# # Resetear las categorías de station_id para eliminar estaciones no presentes
_df_filled["station_id"] = _df_filled["station_id"].cat.remove_unused_categories()

_df_filled.info()

_df_filled.describe()

import matplotlib.pyplot as plt
import seaborn as sns

_df_filled.hist(figsize=(12, 8), bins=50)
plt.show()

_df_filled.dropna().info()

column = 'station_id'

# Crear el gráfico de barras
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
ax = _df_filled.dropna()[column].value_counts().plot(kind='bar')
plt.title(f'Histograma de {column}')
plt.xlabel(column)
plt.ylabel('Frecuencia')
plt.xticks(rotation=90)  # Rotar etiquetas del eje x para mejor legibilidad
plt.tight_layout()  # Ajustar el diseño para evitar superposiciones

# Añadir etiquetas a cada barra
for p in ax.patches:
    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')

plt.show()

# Matriz de correlación
plt.figure(figsize=(10, 6))
# Solo datos numericos
numeric_df = _df_filled.select_dtypes(include=['number'])
sns.heatmap(numeric_df.corr(), annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1, linewidths=0.5)
plt.title("Matriz de Correlación")
plt.show()

import pandas as pd
import numpy as np
import matplotlib.dates as mdates
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler

# Seleccionar columnas numéricas
numerical_cols = _df_filled.columns[2:]

# Aplicar MinMaxScaler por estación
df_normalized = _df_filled.copy()

for station in df_normalized['station_id'].unique():
    mask = df_normalized['station_id'] == station
    scaler = MinMaxScaler()
    df_normalized.loc[mask, numerical_cols] = scaler.fit_transform(df_normalized.loc[mask, numerical_cols])

# Agrupar por mes
df_grouped = df_normalized.groupby(['station_id', pd.Grouper(key='datetime', freq='ME')], observed=False)[numerical_cols].mean().reset_index()

# Graficar cada estación
for station in df_grouped['station_id'].unique():
    station_data = df_grouped[df_grouped['station_id'] == station]

    plt.figure(figsize=(12, 6))

    # Graficar cada contaminante
    for pollutant in numerical_cols:
        sns.lineplot(data=station_data, x='datetime', y=pollutant, label=pollutant)

    # Configurar eje X
    plt.xlim(pd.Timestamp('2000-01-01'), pd.Timestamp('2021-12-31'))
    ax = plt.gca()
    ax.xaxis.set_major_locator(mdates.YearLocator(1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
    plt.xticks(rotation=45)

    # Configuración de gráficos
    plt.title(f"Evolución Temporal de Contaminantes Normalizados - Estación {station}")
    plt.ylabel("Valor Normalizado")
    plt.xlabel("Fecha")
    plt.legend(title="Contaminante", bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
    plt.tight_layout()
    plt.show()

# Aplicar el filtro iterativo
df_filtered = _df_filled[
    _df_filled.apply(lambda row: row['datetime'] >= pd.Timestamp("2010-01-01"), axis=1)
]

df_filtered.info()

# import numpy as np

# df_filtered = df_filtered.copy()

# # Variables cíclicas (para modelos de ML)
# df_filtered.loc[:,'sin_weekday'] = np.sin(2 * np.pi * df_filtered['datetime'].dt.weekday / 7) # 0 = Lunes, 6 = Domingo
# df_filtered.loc[:,'cos_weekday'] = np.cos(2 * np.pi * df_filtered['datetime'].dt.weekday / 7)
# df_filtered.loc[:,'sin_weekofyear'] = np.sin(2 * np.pi * df_filtered['datetime'].dt.isocalendar().week / 52)
# df_filtered.loc[:,'cos_weekofyear'] = np.cos(2 * np.pi * df_filtered['datetime'].dt.isocalendar().week / 52)
# df_filtered.loc[:,'sin_month'] = np.sin(2 * np.pi * df_filtered['datetime'].dt.month / 12)
# df_filtered.loc[:,'cos_month'] = np.cos(2 * np.pi * df_filtered['datetime'].dt.month / 12)

# # Extraer características temporales
# df_filtered.loc[:,'day'] = df_filtered['datetime'].dt.day
# df_filtered.loc[:,'dayofyear'] = df_filtered['datetime'].dt.dayofyear

# Exportar df_filled a un archivo csv
df_filtered.to_csv('/content/drive/MyDrive/SAMSUNG_IA_2/df_filtered.csv', index=False)

"""-----"""

import pandas as pd

# Leer df_filled desde el archivo csv
df_filtered = pd.read_csv('/content/drive/MyDrive/SAMSUNG_IA_2/df_filtered.csv')

# Convertir la columna 'datetime' a tipo de dato datetime
df_filtered['datetime'] = pd.to_datetime(df_filtered['datetime'])

# Verificar el DataFrame cargado
df_filtered.info()

df_filtered.describe()

# Obtener las columnas numéricas a graficar
numerical_cols = ['PM2.5', 'PM10', 'NOx', 'O3', 'CO']

import matplotlib.pyplot as plt

# Obtener estaciones únicas
stations = df_filtered["station_id"].unique()

# Crear histogramas para cada estación
for station in stations:
    station_data = df_filtered[df_filtered["station_id"] == station]

    # Crear una figura con subgráficos para todas las variables numéricas
    fig, axes = plt.subplots(nrows=1, ncols=len(numerical_cols), figsize=(5 * len(numerical_cols), 5))
    fig.suptitle(f"Histogramas de variables para la estación {station}", fontsize=16)

    # Iterar sobre cada variable numérica
    for i, col in enumerate(numerical_cols):
        ax = axes[i] if len(numerical_cols) > 1 else axes
        ax.hist(station_data[col].dropna(), bins=30, alpha=0.7, color="blue", edgecolor="black")
        ax.set_title(col)
        ax.set_xlabel("Valor")
        ax.set_ylabel("Frecuencia")

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Configuración de los gráficos
plt.figure(figsize=(15, 10))

# Crear un gráfico de cajas (boxplot) por estación para cada columna numérica
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(2, 3, i)  # Subgráfico en una grilla de 2 filas, 3 columnas
    sns.boxplot(x='station_id', y=col, data=df_filtered)
    plt.title(f'Distribución de {col} por Estación')
    plt.xticks(rotation=90)  # Rotar las etiquetas de estación para mejor visualización

# Ajustar el layout y mostrar el gráfico
plt.tight_layout()
plt.show()

import os
import joblib
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Copiar DataFrame para no modificar el original
df_normalized= df_filtered.copy()

# Directorio donde se guardarán los modelos
scaler_dir = "/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Normalized_station/"

# Seleccionar solo las columnas a escalar
numerical_cols = ['PM2.5', 'PM10', 'NOx', 'O3', 'CO']

# Iterar sobre cada estación y aplicar MinMaxScaler
for station in df_normalized['station_id'].unique():
    mask = df_normalized['station_id'] == station
    scaler = MinMaxScaler()

    # Ajustar y transformar solo las columnas numéricas
    df_normalized.loc[mask, numerical_cols] = scaler.fit_transform(df_normalized.loc[mask, numerical_cols])

    # Guardar el modelo del scaler para esta estación
    scaler_filename = os.path.join(scaler_dir, f"scaler_station_{station}.pkl")
    joblib.dump(scaler, scaler_filename)

    print(f"Scaler guardado para station_id {station}: {scaler_filename}")

# Configuración de los gráficos
plt.figure(figsize=(15, 10))

# Crear un gráfico de cajas (boxplot) por estación para cada columna numérica
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(2, 3, i)  # Subgráfico en una grilla de 2 filas, 3 columnas
    sns.boxplot(x='station_id', y=col, data=df_normalized)
    plt.title(f'Distribución de {col} por Estación/Normalización')
    plt.xticks(rotation=90)  # Rotar las etiquetas de estación para mejor visualización

# Ajustar el layout y mostrar el gráfico
plt.tight_layout()
plt.show()

# Agrupar por 'station_id' y mes, y calcular el promedio
df_grouped = df_normalized.groupby([pd.Grouper(key='datetime', freq='ME'), 'station_id'])[numerical_cols].mean().reset_index()

# Obtener las estaciones únicas
stations = df_grouped['station_id'].unique()

# Crear un gráfico por cada estación
for station in stations:
    # Filtrar los datos para la estación actual
    df_station = df_grouped[df_grouped['station_id'] == station]

    # Graficar la evolución temporal de cada variable para esta estación
    plt.figure(figsize=(14, 8))

    for col in numerical_cols:
        plt.plot(df_station['datetime'], df_station[col], label=col, linestyle='-')

    # Configurar el rango del eje X
    plt.xlim(pd.Timestamp('2010-01-01'), pd.Timestamp('2021-12-31'))

    # Configurar las etiquetas del eje X para que muestren año por año
    ax = plt.gca()  # Obtener el eje actual
    ax.xaxis.set_major_locator(mdates.YearLocator(1))  # Mostrar una etiqueta por año
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Formato de año (YYYY)

    # Rotar las etiquetas del eje X para mejor legibilidad
    plt.xticks(rotation=45)

    # Añadir título y etiquetas
    plt.title(f"Evolución Temporal de Contaminantes y Variables Meteorológicas - Estación {station}")
    plt.ylabel("Valor Promedio Mensual")
    plt.xlabel("Fecha")

    # Añadir leyenda
    plt.legend(title="Variable", bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
    plt.tight_layout()  # Ajustar el layout para que no se corten las etiquetas

    # Mostrar el gráfico
    plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Iterar sobre cada estación
for station in df_normalized['station_id'].unique():
    station_data = df_normalized[df_normalized['station_id'] == station][numerical_cols]
    plt.figure(figsize=(8, 6))
    sns.heatmap(station_data.corr(), annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1, linewidths=0.5)
    plt.title(f'Matriz de Correlación - Estación {station}')
    plt.show()

df_normalized.info()

df_normalized

df_normalized.to_csv('/content/drive/MyDrive/SAMSUNG_IA_2/df_normalized.csv', index=False)

"""-----

Variables Locales
"""

import pandas as pd

# Leer df_filled desde el archivo csv
df_normalized = pd.read_csv('/content/drive/MyDrive/SAMSUNG_IA_2/df_normalized.csv')

# Convertir la columna 'datetime' a tipo de dato datetime
df_normalized['datetime'] = pd.to_datetime(df_normalized['datetime'])

# Verificar el DataFrame cargado
df_normalized.info()

df_normalized

import numpy as np

# Variables cíclicas (para modelos de ML)
_df_test = pd.DataFrame()
_df_test.loc[:,'sin_weekday'] = np.sin(2 * np.pi * df_normalized['datetime'].dt.weekday / 7) # 0 = Lunes, 6 = Domingo
_df_test.loc[:,'cos_weekday'] = np.cos(2 * np.pi * df_normalized['datetime'].dt.weekday / 7)
_df_test.loc[:,'sin_weekofyear'] = np.sin(2 * np.pi * df_normalized['datetime'].dt.isocalendar().week / 52).astype(float)
_df_test.loc[:,'cos_weekofyear'] = np.cos(2 * np.pi * df_normalized['datetime'].dt.isocalendar().week / 52).astype(float)
_df_test.loc[:,'sin_month'] = np.sin(2 * np.pi * df_normalized['datetime'].dt.month / 12)
_df_test.loc[:,'cos_month'] = np.cos(2 * np.pi * df_normalized['datetime'].dt.month / 12)

# Extraer características temporales y convertirlas a float
_df_test.loc[:, 'day'] = df_normalized['datetime'].dt.day.astype(float)
_df_test.loc[:, 'dayofyear'] = df_normalized['datetime'].dt.dayofyear.astype(float)

from sklearn.preprocessing import MinMaxScaler

# Aplicar MaxMinScaler
scaler_features_local = MinMaxScaler()

# Seleccionar las columnas que serán escaladas
columns_to_scale = [
    'sin_weekday', 'cos_weekday', 'sin_weekofyear', 'cos_weekofyear', 'sin_month',
    'cos_month', 'day', 'dayofyear'
]

# Escalar las columnas seleccionadas
_df_test[columns_to_scale] = scaler_features_local.fit_transform(_df_test[columns_to_scale])

joblib.dump(scaler, '/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Normalized_station/scaler_features_local.pkl')

# concatenar df_test y _df_test
df_concatenated = pd.concat([df_normalized, _df_test], axis=1)

df_concatenated.info()

df_concatenated.describe()

"""----

# Imputación de las estaciones con LSTM **Bidireccional**
"""

import tensorflow as tf

def prepare_datasets(df_test):
    # Dataframe de entrenamiento y validación
    df_train = df_test[(df_test['datetime'] >= '2010-01-01') & (df_test['datetime'] <= '2019-12-31')]
    df_train = df_train.set_index("datetime")

    df_val = df_test[(df_test['datetime'] >= '2020-01-01')]
    df_val = df_val.set_index("datetime")

    # Convertir DataFrame a Dataset de TensorFlow
    tf_dataset_train = tf.data.Dataset.from_tensor_slices(df_train.values)
    tf_dataset_val = tf.data.Dataset.from_tensor_slices(df_val.values)

    # Dataset de ventanas de tiempo
    windowed_dataset_train = tf_dataset_train.window(7, shift=1, drop_remainder=True)
    windowed_dataset_val = tf_dataset_val.window(7, shift=1, drop_remainder=True)

    windowed_dataset_train = windowed_dataset_train.flat_map(lambda window: window.batch(7))
    windowed_dataset_val = windowed_dataset_val.flat_map(lambda window: window.batch(7))

    # Crear la máscara booleana para los valores faltantes
    def create_mask(window):
        mask = ~tf.math.is_nan(window)  # True donde no hay NaN
        return tf.cast(mask, tf.float64)

    # Función para agregar máscara al dataset
    def add_mask_to_window(window):
        mask = create_mask(window)  # Crear máscara para la ventana
        window = tf.where(tf.math.is_nan(window), tf.zeros_like(window), window)  # Reemplazar NaN por ceros
        return (window, mask), window  # Devolvemos la ventana y la máscara como tupla

    # Aplicar la función de máscara a los datasets
    windowed_dataset_train = windowed_dataset_train.map(add_mask_to_window)
    windowed_dataset_val = windowed_dataset_val.map(add_mask_to_window)

    # Agrupar en lotes
    windowed_dataset_train = windowed_dataset_train.batch(128)
    windowed_dataset_val = windowed_dataset_val.batch(128)

    # Verificar la forma de los datos antes de entrenar
    for data, label in windowed_dataset_train.take(1):
        print(f"Input shapes: {data[0].shape}, {data[1].shape}")
        print(f"Output shape: {label[0].shape}")

    return windowed_dataset_train, windowed_dataset_val

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.initializers import HeNormal, GlorotUniform
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard

class AutoencoderTrainerImputador:
    def __init__(self, path):
        self.path = path
        self.model = self.build_model()
        self.callbacks = self.get_callbacks()

    # Definir el modelo autoencoder
    def build_model(self):

      # Definir el modelo autoencoder
      input_layer = layers.Input(shape=(7, 13))  # Entrada: 7 pasos de tiempo y 14 características
      mask_input = layers.Input(shape=(7, 13))  # Entrada adicional: máscara booleana

      # --- ENCODER ---
      encoded = layers.Bidirectional(layers.LSTM(128, activation='relu', return_sequences=True,
                                                kernel_initializer=GlorotUniform()))(input_layer)
      encoded = layers.BatchNormalization()(encoded)  # Normalización
      encoded = layers.Dropout(0.2)(encoded)  # Regularización

      encoded = layers.Bidirectional(layers.LSTM(64, activation='relu', return_sequences=False,
                                                kernel_initializer=GlorotUniform()))(encoded)

      # Latent space (representación comprimida)
      latent_space = layers.Dense(64, activation='relu', kernel_initializer=HeNormal())(encoded)
      latent_space = layers.Dense(32, activation='relu', kernel_initializer=HeNormal())(latent_space)  # Capa adicional

      # --- DECODER ---
      decoded = layers.RepeatVector(7)(latent_space)  # Repetimos la representación comprimida
      decoded = layers.Bidirectional(layers.LSTM(64, activation='relu', return_sequences=True,
                                                kernel_initializer=GlorotUniform()))(decoded)
      decoded = layers.BatchNormalization()(decoded)
      decoded = layers.Dropout(0.2)(decoded)

      decoded = layers.Bidirectional(layers.LSTM(128, activation='relu', return_sequences=True,
                                                kernel_initializer=GlorotUniform()))(decoded)

      # Salida: reconstrucción de las 14 características
      output_layer = layers.TimeDistributed(layers.Dense(13, kernel_initializer=GlorotUniform()))(decoded)

      # Asegurar que la salida esté entre 0 y 1 (activación sigmoide)
      output_layer_sigmoid = layers.Activation('sigmoid')(output_layer)

      # Aplicar la máscara a la salida para ignorar los valores faltantes
      masked_output = layers.Multiply()([output_layer_sigmoid, mask_input])  # Multiplicamos por la máscara para ignorar valores faltantes

      # Modelo autoencoder
      autoencoder = models.Model(inputs=[input_layer, mask_input], outputs=masked_output)

      # Compilar el modelo
      autoencoder.compile(optimizer=Adam(learning_rate=0.01), loss='mse')  # Se reduce el LR para estabilizar el entrenamiento

      # Resumen del modelo
      autoencoder.summary()

      return autoencoder

    def get_callbacks(self):
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True,
            start_from_epoch=10,
            verbose=1)

        reduce_lr = ReduceLROnPlateau(
            monitor='loss',
            factor=0.8,
            patience=2,
            min_lr=1e-7,
            verbose=1)

        model_checkpoint = ModelCheckpoint(
            f'{self.path}/best_autoencoder.h5',
            monitor='val_loss',
            save_best_only=True,
            verbose=0)

        tensorboard = TensorBoard(
            log_dir=f'{self.path}/logs',
            histogram_freq=1,
            write_graph=True)

        return [reduce_lr, model_checkpoint, early_stopping, tensorboard]

    def train(self, windowed_dataset_train, windowed_dataset_val, epochs=100,batch_size=128):
        history = self.model.fit(
            windowed_dataset_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=windowed_dataset_val,
            callbacks=self.callbacks
        )
        return history

    def predict(self, data):
        return self.model.predict(data)

# Convertir en ventanas de tamaño 7
def create_sequences(data, mask, seq_length=7):
    sequences = []
    mask_sequences = []
    for i in range(len(data) - seq_length + 1):
        sequences.append(data.iloc[i:i+seq_length].values)
        mask_sequences.append(mask[i:i+seq_length])
    return np.array(sequences), np.array(mask_sequences)


def preprocess_missing_values(df, sequence_length):

    a = df.drop(columns=['datetime']).copy()

    # Crear la máscara de valores faltantes (1 = valor presente, 0 = faltante)
    mask = np.where(a.isna(), 0, 1)

    # Reemplazar los NaN en los datos numéricos con 0
    a.fillna(0, inplace=True)

    # Se pasa el inverso de la máscara (1-mask) para predecir valores faltantes
    X_test, mask_test = create_sequences(a, 1 - mask, sequence_length)

    # Asegurar que las dimensiones coinciden
    X_test = X_test.reshape((-1, 7, 13))
    mask_test = mask_test.reshape((-1, 7, 13))

    return X_test, mask_test

def reconstruct_from_sequences(sequences):
    seq_length = sequences.shape[1]  # Longitud de la ventana (7)
    n_features = sequences.shape[2]  # Número de características
    total_length = len(sequences) + seq_length - 1  # Tamaño del DataFrame original

    reconstructed = np.zeros((total_length, n_features))
    counts = np.zeros((total_length, n_features))

    for i in range(len(sequences)):
        reconstructed[i:i+seq_length] += np.nan_to_num(sequences[i])  # Evita NaN en sumas
        counts[i:i+seq_length] += 1

    # Evitar división por 0
    reconstructed = np.divide(reconstructed, counts, out=np.zeros_like(reconstructed), where=counts > 0)

    return pd.DataFrame(reconstructed)

import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import pandas as pd
import matplotlib.cm as cm

def plot_time_series_features(df_test, df_imputed, station_id):
    df_test['datetime'] = pd.to_datetime(df_test['datetime'])
    df_imputed['datetime'] = pd.to_datetime(df_imputed['datetime'])

    # Definir las columnas numéricas relevantes (contaminantes y variables meteorológicas)
    numerical_cols = df_imputed.columns[1:6]

    # Crear un colormap para asignar colores únicos a cada característica
    cmap = plt.colormaps['tab10']  # 'tab10' tiene 10 colores diferentes
    colors = [cmap(i) for i in range(len(numerical_cols))]

    # Crear la figura y los subplots (un subplot para cada variable)
    fig, axes = plt.subplots(len(numerical_cols), 1, figsize=(30, 15), sharex=True)

    # Si hay solo una variable, axes será un solo eje, no una lista
    if len(numerical_cols) == 1:
        axes = [axes]

    # Graficar cada variable en su propio eje
    for i, col in enumerate(numerical_cols):
        axes[i].plot(df_test['datetime'], df_test[col], label=f'Original - {col}', color=colors[i], linestyle='-', alpha=0.7)
        axes[i].plot(df_imputed['datetime'], df_imputed[col], label=f'Imputada - {col}', color=colors[i], linestyle='--', alpha=0.7)

        # Configurar los ejes
        axes[i].set_title(f"{col} - Estación {station_id}")
        axes[i].set_ylabel("Valor")
        axes[i].xaxis.set_major_locator(mdates.YearLocator(1))
        axes[i].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
        axes[i].tick_params(axis='x', rotation=45)

        # Añadir leyenda
        axes[i].legend(loc='upper left')

    # Añadir título y etiquetas generales
    plt.xlabel("Fecha")

    # Ajustar el layout para evitar el solapamiento de etiquetas
    plt.tight_layout()
    plt.show()


def plot_time_series(df_test, df_imputed, station_id):
    df_test['datetime'] = pd.to_datetime(df_test['datetime'])
    df_imputed['datetime'] = pd.to_datetime(df_imputed['datetime'])

    # Definir las columnas numéricas relevantes (contaminantes y variables meteorológicas)
    numerical_cols = df_imputed.columns[1:6]

    # Agrupar por mes y calcular el promedio
    df_grouped_test = df_test.groupby(pd.Grouper(key='datetime', freq='ME'))[numerical_cols].mean().reset_index()
    df_grouped_imputed = df_imputed.groupby(pd.Grouper(key='datetime', freq='ME'))[numerical_cols].mean().reset_index()

    # Crear la figura y los subplots
    fig, axes = plt.subplots(1, 2, figsize=(22, 8), sharey=True)

    # Graficar la serie original en el primer subplot
    for col in numerical_cols:
        axes[0].plot(df_grouped_test['datetime'], df_grouped_test[col], label=f'Original - {col}', linestyle='-', alpha=0.7)

    # Graficar la serie imputada en el segundo subplot
    for col in numerical_cols:
        axes[1].plot(df_grouped_imputed['datetime'], df_grouped_imputed[col], label=f'Imputada - {col}', linestyle='-', alpha=0.7)

    # Configurar los ejes X (fechas) para ambos subplots
    for ax in axes:
        ax.xaxis.set_major_locator(mdates.YearLocator(1))
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
        ax.tick_params(axis='x', rotation=45)

    # Añadir títulos y etiquetas a cada subplot
    axes[0].set_title(f"Serie Original de Contaminantes y Variables Meteorológicas - Estación {station_id}")
    axes[1].set_title(f"Serie Imputada de Contaminantes y Variables Meteorológicas - Estación {station_id}")

    for ax in axes:
        ax.set_xlabel("Fecha")
        ax.set_ylabel("Valor Promedio Mensual")

    # Mostrar la leyenda solo una vez para ambos subplots
    handles, labels = axes[0].get_legend_handles_labels()
    axes[1].legend(handles, numerical_cols, title="Variable", bbox_to_anchor=(1.05, 1), loc='upper left', ncol=1)

    # Ajustar el layout para evitar el solapamiento de etiquetas
    plt.tight_layout()
    plt.show()

# Código de colores ANSI
RED = '\033[31m'
GREEN = '\033[32m'
YELLOW = '\033[33m'
BLUE = '\033[34m'
RESET = '\033[0m'

# Lista de estaciones a procesar
stations = df_concatenated['station_id'].unique()

# Crear DataFrame vacío para almacenar imputaciones
df_imputed_list = []  # Usamos una lista para eficiencia

for station in stations:

    # Filtrar datos para la estación actual
    df_test = df_concatenated[df_concatenated['station_id'] == station]
    df_test = df_test.drop(columns=['station_id'])

    # Mostrar información
    print(f"Datos para estación {RED}{station}{RESET}:")
    print(df_test.info())
    print(f"{YELLOW}Descripción de los datos para estación {station}:{RESET}")
    print(df_test.describe())

    # Preparar datasets de entrenamiento y validación
    windowed_dataset_train, windowed_dataset_val = prepare_datasets(df_test)

    # Entrenar modelo
    autoencoder = AutoencoderTrainerImputador(f'/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Imputadores_2/{station}')
    history = autoencoder.train(windowed_dataset_train, windowed_dataset_val)

    X_test, mask_test = preprocess_missing_values(df_test, 7)

    print(f"\n{YELLOW}Predicción de los datos faltantes:{RESET}")
    # Predecir los valores reconstruidos
    X_pred = autoencoder.predict([X_test, mask_test])

    print("\n")

    # Se revierten las ventanas
    reconstructed_df = reconstruct_from_sequences(X_pred)
    reconstructed_df.columns = df_test.columns[1:]

    # Reindexamos reconstructed_df para que coincida con df_test
    reconstructed_df = reconstructed_df.set_index(df_test.index)

    print(f"{GREEN}Forma de los datos originales: {df_test.shape}{RESET}")
    print(df_test)
    print(f"{RED}Forma de los datos reconstruidos: {reconstructed_df.shape}{RESET}")
    print(reconstructed_df)

    # Imputación de datos vacios
    df_station_imputed = df_test.copy()
    df_station_imputed.iloc[:, 1:] = df_station_imputed.iloc[:, 1:].combine_first(reconstructed_df)
    df_station_imputed['station_id'] = station  # Restaurar station_id

    print(f"{BLUE}Forma de los datos imputados: {df_station_imputed.shape}{RESET}")
    print(df_station_imputed)
    print(f"{YELLOW}Descripción de los datos imputados para estación {station}:{RESET}")
    print(df_station_imputed.describe())

    plot_time_series(df_test, df_station_imputed, station)
    plot_time_series_features(df_test, df_station_imputed, station)

    # Agregar el resultado a la lista
    df_imputed_list.append(df_station_imputed)

    print(f"Finzalizado {station}")
    print("-"*200)


# Concatenar resultados de todas las estaciones
df_imputed = pd.concat(df_imputed_list, ignore_index=True)

# Guardar archivo con todas las imputaciones
df_imputed.to_csv('/content/drive/MyDrive/SAMSUNG_IA_2/df_imputed_full.csv', index=False)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir /content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Imputadores/full/Autoencoder/logs

"""Normalización inversa

"""

import pandas as pd

# Cargar el archivo CSV en un DataFrame
df_imputed_full = pd.read_csv('/content/drive/MyDrive/SAMSUNG_IA_2/df_imputed_full.csv')
df_imputed_full

df_imputed_full.describe()

numerical_cols = ['datetime', 'PM2.5', 'PM10', 'NOx', 'O3', 'CO']
numerical_cols[1:]

import os
import joblib
import pandas as pd

# Directorio donde están guardados los scalers
scaler_dir = "/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Normalized_station/"

# Columnas numéricas a transformar
numerical_cols = ['datetime', 'station_id', 'PM2.5', 'PM10', 'NOx', 'O3', 'CO']

# Lista para almacenar los DataFrames desnormalizados
dfs_denormalized = []

# Iterar sobre cada estación y aplicar la transformación inversa
for station in df_imputed_full['station_id'].unique():

    scaler_filename = scaler_dir+f"scaler_station_{station}.pkl"

    # Cargar el scaler correspondiente
    scaler = joblib.load(scaler_filename)

    # Filtrar solo los datos de esta estación
    df_station = df_imputed_full[df_imputed_full['station_id'] == station][numerical_cols].copy()

    # Aplicar la transformación inversa
    df_station[numerical_cols[2:]] = scaler.inverse_transform(df_station[numerical_cols[2:]])

    # Agregar el DataFrame desnormalizado a la lista
    dfs_denormalized.append(df_station)

    print(f"Normalización inversa aplicada para station_id {station}")

# Concatenar todos los DataFrames
df_final = pd.concat(dfs_denormalized, ignore_index=True)

# # Guardar el DataFrame en un CSV
# output_csv_path = "/content/drive/MyDrive/SAMSUNG_IA_2/df_denormalized.csv"
# df_final.to_csv(output_csv_path, index=False)

df_final.info()

df_final

df_final.describe()

# Guardar el DataFrame en un CSV
output_csv_path = "/content/drive/MyDrive/SAMSUNG_IA_2/df_imputed_full_denormalized.csv"
df_final.to_csv(output_csv_path, index=False)

"""----"""

import pandas as pd

# Cargar el archivo CSV en un DataFrame
df_imputed_full = pd.read_csv('/content/drive/MyDrive/SAMSUNG_IA_2/df_imputed_full.csv')
df_imputed_full

import	matplotlib.pyplot as plt
import seaborn as sns

# Definir las columnas numéricas relevantes (contaminantes y variables meteorológicas)
numerical_cols = df_imputed_full.columns[1:6]

# Configuración de los gráficos
plt.figure(figsize=(15, 10))

# Crear un gráfico de cajas (boxplot) por estación para cada columna numérica
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(2, 3, i)  # Subgráfico en una grilla de 2 filas, 3 columnas
    sns.boxplot(x='station_id', y=col, data=df_imputed_full)
    plt.title(f'Distribución de {col} por Estación/Normalización')
    plt.xticks(rotation=90)  # Rotar las etiquetas de estación para mejor visualización

# Ajustar el layout y mostrar el gráfico
plt.tight_layout()
plt.show()

"""------

#Regresión serie temporal
"""

import pandas as pd

# Cargar el archivo CSV en un DataFrame
df_imputed_full = pd.read_csv('/content/drive/MyDrive/SAMSUNG_IA_2/df_imputed_full.csv')
df_imputed_full

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.initializers import HeNormal
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard

class AutoencoderTrainerRegressor:
    def __init__(self, path):
        self.path = path
        self.model = self.build_model()
        self.callbacks = self.get_callbacks()

    def build_model(self):
        input_layer = layers.Input(shape=(7, 13))  # 7 pasos de tiempo, 13 características

        # --- ENCODER ---
        encoded = layers.Bidirectional(layers.LSTM(128, activation='relu', return_sequences=True))(input_layer)
        encoded = layers.BatchNormalization()(encoded)
        encoded = layers.Dropout(0.2)(encoded)

        encoded = layers.Bidirectional(layers.LSTM(64, activation='relu', return_sequences=False))(encoded)

        # Latent space (respresentación comprimida)
        latent_space = layers.Dense(64, activation='relu', kernel_initializer=HeNormal())(encoded)
        latent_space = layers.Dense(32, activation='relu', kernel_initializer=HeNormal())(encoded)

        # Salida: Predicción de las 13 características en el último paso
        output_layer = layers.Dense(5, activation='sigmoid')(latent_space)

        # Modelo
        autoencoder = models.Model(inputs=input_layer, outputs=output_layer)

        # Compilación
        autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

        autoencoder.summary()

        return autoencoder

    def get_callbacks(self):
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=6,
            restore_best_weights=True,
            start_from_epoch=10,
            verbose=1)

        reduce_lr = ReduceLROnPlateau(
            monitor='loss',
            factor=0.8,
            patience=2,
            min_lr=1e-7,
            verbose=1)

        model_checkpoint = ModelCheckpoint(
            f'{self.path}/best_autoencoder.h5',
            monitor='val_loss',
            save_best_only=True,
            verbose=0)

        tensorboard = TensorBoard(
            log_dir=f'{self.path}/logs',
            histogram_freq=1,
            write_graph=True)

        return [reduce_lr, model_checkpoint, early_stopping, tensorboard]

    def train(self, windowed_dataset_train, windowed_dataset_val, epochs=100,batch_size=128):
        history = self.model.fit(
            windowed_dataset_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=windowed_dataset_val,
            callbacks=self.callbacks
        )
        return history

    def predict(self, data):
        return self.model.predict(data)

# Código de colores ANSI
RED = '\033[31m'
GREEN = '\033[32m'
YELLOW = '\033[33m'
BLUE = '\033[34m'
RESET = '\033[0m'

# Lista de estaciones a procesar
stations = df_imputed_full['station_id'].unique()

for station in stations:

    # Filtrar datos para la estación actual
    df_test = df_imputed_full[df_imputed_full['station_id'] == station]
    df_test = df_test.drop(columns=['station_id'])

    # Mostrar información
    print(f"Datos para estación {RED}{station}{RESET}:")
    print(df_test.info())
    print(f"{YELLOW}Descripción de los datos para estación {station}:{RESET}")
    print(df_test.describe())

    # Preparar datasets de entrenamiento y validación
    df_train = df_test[(df_test['datetime'] >= '2010-01-01') & (df_test['datetime'] <= '2019-12-31')]
    df_train = df_train.set_index("datetime")

    df_val = df_test[(df_test['datetime'] >= '2020-01-01')]
    df_val = df_val.set_index("datetime")

    # Convertir DataFrame a Dataset de TensorFlow
    tf_dataset_train = tf.data.Dataset.from_tensor_slices(df_train.values)
    tf_dataset_val = tf.data.Dataset.from_tensor_slices(df_val.values)

    # Dataset de ventanas de tiempo
    windowed_dataset_train = tf_dataset_train.window(8, shift=1, drop_remainder=True)
    windowed_dataset_val = tf_dataset_val.window(8, shift=1, drop_remainder=True)

    windowed_dataset_train = windowed_dataset_train.flat_map(lambda window: window.batch(8))
    windowed_dataset_val = windowed_dataset_val.flat_map(lambda window: window.batch(8))

    # Etiquetas ultima dia
    windowed_dataset_train = windowed_dataset_train.map(lambda window: (window[:-1, :], window[-1, :5]))
    windowed_dataset_val = windowed_dataset_val.map(lambda window: (window[:-1, :], window[-1, :5]))

    # Aumento la dimensionalidad
    windowed_dataset_train = windowed_dataset_train.batch(128)
    windowed_dataset_val = windowed_dataset_val.batch(128)

    # Verificar la forma de los datos antes de entrenar
    for data, label in windowed_dataset_train.take(1):
        data_shape = data.shape
        label_shape = label.shape
        print("Forma de los datos:", data_shape)
        print("Forma de las etiquetas:", label_shape)

    # Entrenar modelo
    autoencoder = AutoencoderTrainerRegressor(path=f'/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Regresiones_2/{station}')
    history = autoencoder.train(windowed_dataset_train, windowed_dataset_val)

    print(f"Finzalizado {station}")
    print("-"*200)

df_imputed_full[df_imputed_full['station_id']==102].iloc[-10:-2, :].iloc[:,1:-1]

import numpy as np
import tensorflow as tf
from tensorflow.keras.losses import MeanSquaredError


# Cargar el modelo guardado
modelo_cargado = tf.keras.models.load_model(
    '/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Regresiones_2/102/best_autoencoder.h5',
    custom_objects={'mse': MeanSquaredError()})


# Generar un tensor aleatorio con la misma forma que los datos de entrada (batch_size, 7, 13)
batch_size = 1  # Puedes cambiar el tamaño del batch
X_input = df_imputed_full[df_imputed_full['station_id']==102].iloc[-10:-3, :].iloc[:,1:-1].values
X_input = np.expand_dims(X_input, axis=0)  # Agrega una dimensión en la posición 0
print(X_input.shape)

# Realizar una predicción
prediccion = modelo_cargado.predict(X_input)

# Imprimir la predicción
print("Predicción:")
print(prediccion)

"""#Clasificador"""

import pandas as pd

# Cargar el archivo CSV en un DataFrame
df_imputed_full = pd.read_csv('/content/drive/MyDrive/SAMSUNG_IA_2/df_imputed_full.csv')
df_imputed_full = df_imputed_full.sort_values(by=['datetime', 'station_id'], ascending=[True, True])

df_imputed_full.columns[1:-1]

from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from matplotlib.colors import ListedColormap


def target_kmeans(df, seed=43):
  # Seleccionar características para clustering
    features = ['PM2.5', 'PM10', 'NOx', 'O3', 'CO', 'sin_weekday', 'cos_weekday',
                'sin_weekofyear', 'cos_weekofyear', 'sin_month', 'cos_month', 'day', 'dayofyear']

    X = df[features].values

    # Determinar el mejor número de clusters con el coeficiente de silueta
    silhouette_scores = []
    k_values = range(2, 8)

    for k in k_values:
        kmeans = MiniBatchKMeans(n_clusters=k, random_state=seed, n_init=10)
        labels = kmeans.fit_predict(X)
        score = silhouette_score(X, labels)
        silhouette_scores.append(score)

    # Plot de Silhouette Score vs K
    plt.figure(figsize=(8, 5))
    plt.plot(k_values, silhouette_scores, marker='o', linestyle='-', color='blue')
    plt.xlabel('Número de Clusters (K)')
    plt.ylabel('Coeficiente de Silueta')
    plt.title(f'Silueta Score vs K - Estación {station}')
    plt.grid()
    plt.show()

    # Seleccionar el mejor K
    best_k = k_values[np.argmax(silhouette_scores)]
    print(f'Mejor número de clusters para estación {station}: {best_k}')

    # Aplicar MiniBatchKMeans con el mejor K en el espacio original
    kmeans = MiniBatchKMeans(n_clusters=best_k, random_state=seed, n_init=10)
    df['target_kmeans'] = kmeans.fit_predict(X)

    # Imprimir conteo de instancias por cluster
    print(df['target_kmeans'].value_counts())
    print(df['target_kmeans'].unique())


    # Aplicar PCA para reducir a 2 dimensiones
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)

    # Aplicar KMeans en los datos originales
    labels = df['target_kmeans']
    centroids = pca.transform(kmeans.cluster_centers_)  # Transformar los centroides a 2D

    # Crear una malla de puntos para el área de decisión
    h = .02  # Tamaño del paso en la malla
    x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1
    y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # Predecir en cada punto de la malla
    Z = kmeans.predict(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()]))
    Z = Z.reshape(xx.shape)

    # Colores
    cmap = ListedColormap(sns.color_palette("tab10", best_k).as_hex())

    # Plotear la decisión y los datos
    plt.figure(figsize=(10, 6))
    plt.contourf(xx, yy, Z, cmap=cmap, alpha=0.3)  # Límites de decisión
    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap=cmap, edgecolor='k', alpha=0.7)  # Datos
    plt.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=200, c='red', label='Centroides')  # Centroides


    # Crear leyenda para los clusters
    handles, labels_legend = scatter.legend_elements()
    plt.legend(handles, [f'Cluster {i}' for i in range(best_k)], title="Clusters")

    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.title(f'Clusters y límites de decisión con PCA - Estación {station}')
    plt.grid()
    plt.show()

    return df['target_kmeans']

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from scipy.stats import randint
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import os

RED = '\033[31m'
GREEN = '\033[32m'
YELLOW = '\033[33m'
BLUE = '\033[34m'
RESET = '\033[0m'

for station in df_imputed_full['station_id'].unique():

    df = df_imputed_full[df_imputed_full['station_id'] == station].copy()
    # Seleccionar características
    features = ['PM2.5', 'PM10', 'NOx', 'O3', 'CO', 'sin_weekday', 'cos_weekday',
                'sin_weekofyear', 'cos_weekofyear', 'sin_month', 'cos_month', 'day', 'dayofyear']

    X = df[features]
    y = target_kmeans(df, seed=41)

    print(f"\n")
    # Crear el histograma
    plt.figure(figsize=(8, 5))
    ax = sns.histplot(y, discrete=True)

    # Mostrar el número de instancias sobre cada barra
    for p in ax.patches:
        ax.text(p.get_x() + p.get_width() / 2,  # Posición en X
                p.get_height() + 1,  # Posición en Y
                int(p.get_height()),  # Texto con el número de instancias
                ha='center', va='bottom', fontsize=12, fontweight='bold', color='black')

    # Personalizar el gráfico
    plt.xlabel('Categoría')
    plt.ylabel('Frecuencia')
    plt.title(f'Histograma de target - Estación {station}')
    plt.grid(True)

    # Mostrar
    plt.show()

    # Normalización de los datos
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Aplicar PCA para reducción de dimensionalidad
    pca = PCA(n_components=0.95)  # Mantener 95% de varianza
    X_pca = pca.fit_transform(X_scaled)

    # Matriz de correlación orginal
    print(f"\n")
    plt.figure(figsize=(8, 6))
    sns.heatmap(X.corr(), annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1, linewidths=0.5)
    plt.title(f'Matriz de Correlación - Estación {station}')
    plt.show()

    # Matriz de correlación PCA
    print(f"\n")
    plt.figure(figsize=(9, 6))
    sns.heatmap(pd.DataFrame(X_pca).corr(), annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1, linewidths=0.5)
    plt.title(f'Matriz de Correlación X_pca - Estación {station}')
    plt.show()

    print(f"\n{BLUE}Número de características originales: {X.shape[1]} después de PCA: {X_pca.shape[1]} {RESET}")
    print(f"\nVarianza explicada al 95% del total por cada componente:\n {pca.explained_variance_ratio_.reshape(-1, 1)}", "\n")


    # Dividir datos en entrenamiento y prueba
    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42, stratify=y)
    print(f"\n{YELLOW}Tamaño del conjunto de entrenamiento: {X_train.shape[0]}{RESET}")
    print(f"{YELLOW}Tamaño del conjunto de prueba: {X_test.shape[0]}{RESET}", "\n")
    print(f"\n")



    # RandomForestClassifier
    param_rf = {
        "n_estimators": randint(5, 50),
        "max_depth": randint(3, 10),
    }

    rf = RandomizedSearchCV(
        RandomForestClassifier(random_state=42),
        param_rf,
        n_iter=10,
        cv=5,
        random_state=42,
        n_jobs=-1,
    )

    rf.fit(X_train, y_train)
    print(f"Mejores hiperparámetros para Random Forest: {rf.best_params_}")

    # SVC
    param_svm = {
        "C": [0.1, 1, 10],
        "gamma": [0.001, 0.01, 0.1],
        "kernel": ['rbf', 'linear'],
    }

    svm = RandomizedSearchCV(
        SVC(probability=True, random_state=42),
        param_svm,
        n_iter=10,
        cv=5,
        random_state=42,
        n_jobs=-1,
    )

    svm.fit(X_train, y_train)
    print(f"Mejores hiperparámetros para SVM: {svm.best_params_}")

    # Regresión Logística
    param_lr = {
        "C": [0.1, 1, 10],
        "solver": ['lbfgs', 'liblinear'],
        "max_iter": [50, 100]
    }

    lr = RandomizedSearchCV(
        LogisticRegression(random_state=42),
        param_lr,
        n_iter=10,
        cv=5,
        random_state=42,
        n_jobs=-1,
    )

    lr.fit(X_train, y_train)
    print(f"Mejores hiperparámetros para Regresión Logística: {lr.best_params_}")

    # Ensamble con Stacking
    estimators = [
        ('rf', rf.best_estimator_),
        ('svm', svm.best_estimator_),
        ('lr', lr.best_estimator_)
    ]

    stk = StackingClassifier(
        estimators=estimators,
        final_estimator=LogisticRegression(penalty='l2', C=1.0),  # Regularización en el meta-modelo
        n_jobs=-1,
        verbose=3,
        passthrough=True
    )

    stk.fit(X_train, y_train)
    print(f"Precisión del modelo Stacking: {stk.score(X_test, y_test)}")

    # Predicciones para cada modelo
    y_pred_stk = stk.predict(X_test)

    # Calcular la matriz de confusión
    cm = confusion_matrix(y_test, y_pred_stk)

    # Graficar la matriz de confusión con seaborn
    print(f"\n")
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt='d', cmap="Blues", xticklabels=set(y_test), yticklabels=set(y_pred_stk))
    plt.xlabel('Predicción')
    plt.ylabel('Real')
    plt.title(f'Matriz de Confusión - Estación {station}')
    plt.show()

    print(f"\n")
    # Mostrar el classification_report para cada modelo
    print("Classification Report - Stacking Ensemble:")
    print(classification_report(y_test, y_pred_stk, digits=5))

    # Crear la carpeta si no existe
    save_path = f'/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Clasificadores_2/{station}/'
    os.makedirs(save_path, exist_ok=True)
    # Guardar el modelo correctamente
    joblib.dump(scaler, os.path.join(save_path, 'scaler_model.pkl'))
    joblib.dump(pca, os.path.join(save_path, 'pca_model.pkl'))
    joblib.dump(stk, os.path.join(save_path, 'stk_model.pkl'))

    print(f"Modelo guardado para estación {station}")
    print("-"*200)

import joblib

# Cargar el modelo de componentes principales
pca_loaded = joblib.load("/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Clasificadores_2/102/pca_model.pkl")

# Cargar el modelo clasificador
stk_loaded = joblib.load("/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Clasificadores_2/102/stk_model.pkl")


# Imprimir resultados
print(stk_loaded)
print(pca_loaded)

df_imputed_full.iloc[1:2, :].iloc[:, 1:-1]

a = pca_loaded.transform(df_imputed_full.iloc[-1:, :].iloc[:, 1:-1])
a

stk_loaded.predict(a)

"""-----

#Pipeline
"""

import pandas as pd
import numpy as np

df_imputed_full_denormalized = pd.read_csv('/content/drive/MyDrive/SAMSUNG_IA_2/df_imputed_full_denormalized.csv')
df_imputed_full_denormalized['state'] = np.nan

# Asegúrate de que 'datetime' esté en formato de fecha
df_imputed_full_denormalized['datetime'] = pd.to_datetime(df_imputed_full_denormalized['datetime'])

df_imputed_full_denormalized.describe()

import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import pandas as pd


def plot_time_series(df, numerical_cols, station):
    plt.figure(figsize=(20, 8))

    # Graficar cada columna numérica
    for col in numerical_cols:
        plt.plot(df['datetime'], df[col], label=col, linestyle='-')

    # Configurar el rango del eje X
    ax = plt.gca()
    ax.set_xlim(df['datetime'].min(), df['datetime'].max())

    # Configurar las etiquetas del eje X
    ax.xaxis.set_major_locator(mdates.DayLocator())
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

    # Rotar etiquetas y ajustar posición
    plt.xticks(rotation=90, ha='right')  # ha='right' alinea mejor las etiquetas rotadas

    # Ajustar márgenes
    plt.subplots_adjust(bottom=0.25)  # Más espacio en la parte inferior
    plt.margins(x=0.02)  # Pequeño margen a los lados

    # Título y etiquetas
    plt.title(f"Evolución Temporal de Contaminantes y Variables Meteorológicas - Estación {station}", pad=20)
    plt.ylabel("Valor Promedio Mensual")
    plt.xlabel("Fecha", labelpad=10)

    # Leyenda
    plt.legend(title="Variable", bbox_to_anchor=(1.05, 1), loc='upper left')

    # Ajuste final
    plt.tight_layout(rect=[0, 0.05, 0.95, 1])  # Deja espacio para la leyenda y etiquetas

    plt.show()

import numpy as np
import pandas as pd

def extract_cyclic_features(df):
    # mode:
    #     - "cyclic" -> Solo senos, cosenos y dias
    #     - "days" -> Solo day y dayofyear

    _df_test = pd.DataFrame()

    # Sin y coseno de los días de la semana
    _df_test.loc[:, 'sin_weekday'] = np.sin(2 * np.pi * df['datetime'].dt.weekday / 7)
    _df_test.loc[:, 'cos_weekday'] = np.cos(2 * np.pi * df['datetime'].dt.weekday / 7)

    # Sin y coseno de la semana del año
    _df_test.loc[:, 'sin_weekofyear'] = np.sin(2 * np.pi * df['datetime'].dt.isocalendar().week / 52).astype(float)
    _df_test.loc[:, 'cos_weekofyear'] = np.cos(2 * np.pi * df['datetime'].dt.isocalendar().week / 52).astype(float)

    # Sin y coseno del mes
    _df_test.loc[:, 'sin_month'] = np.sin(2 * np.pi * df['datetime'].dt.month / 12)
    _df_test.loc[:, 'cos_month'] = np.cos(2 * np.pi * df['datetime'].dt.month / 12)

    # Extraer características temporales
    _df_test.loc[:, 'day'] = df['datetime'].dt.day.astype(float)
    _df_test.loc[:, 'dayofyear'] = df['datetime'].dt.dayofyear.astype(float)

    df = df.drop(columns=['datetime'])
    # Concatenar las nuevas características con el DataFrame original
    df = pd.concat([df, _df_test], axis=1)

    return df

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import load_model
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler, FunctionTransformer
from tensorflow.keras.losses import MeanSquaredError


# Cargar el DataFrame (asegúrate de que df_imputed_full_denormalized esté disponible)
station = 102
df = df_imputed_full_denormalized[df_imputed_full_denormalized['station_id'] == station].copy()

# Seleccionar solo el último mes
last_month = df['datetime'].max() - pd.DateOffset(months=1)  # El primer día del mes anterior
df_last_month = df[df['datetime'] > last_month]

# Normalizar las columnas numéricas (a partir de la columna 2, excepto state)
df_last_month_ = df_last_month.copy()
df_last_month_.iloc[:, 2:-1] = MinMaxScaler().fit_transform(df_last_month.iloc[:, 2:-1])
numerical_cols = ['PM2.5', 'PM10', 'NOx', 'O3', 'CO']
plot_time_series(df_last_month_, numerical_cols, station)

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import load_model
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.losses import MeanSquaredError
import joblib
from datetime import datetime

print("Iniciando pipeline de predicción para estación 102...")

# Cargar el DataFrame con la estación 102
station = 102
df = df_imputed_full_denormalized[df_imputed_full_denormalized['station_id'] == station]
print(f"DataFrame cargado con {df.shape[0]} filas para la estación {station}")

# Cargar modelos entrenados
print("Cargando modelo autoencoder...")
regressor = tf.keras.models.load_model(
    f'/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Regresiones_2/{station}/best_autoencoder.h5',
    custom_objects={'mse': MeanSquaredError}
)

print("Cargando modelo PCA y clasificador...")
pca_loaded = joblib.load(f'/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Clasificadores_2/{station}/pca_model.pkl')
stk_loaded = joblib.load(f'/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Clasificadores_2/{station}/stk_model.pkl')

print("Cargando escaladores...")
scaler_features = joblib.load(f'/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Normalized_station/scaler_station_{station}.pkl')
scaler_features_local = joblib.load(f'/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Normalized_station/scaler_features_local.pkl')

# Convertir cadena a datetime
start_date = datetime.strptime("2022-01-01", "%Y-%m-%d")
end_date = datetime.strptime("2022-01-31", "%Y-%m-%d")

print("Iniciando predicciones por día...")
for i in range((end_date - start_date).days + 1):

    print(200*'-')
    print(f"\n--- Entrada {i + 1} ---")

    df_ = df[-7:].copy()
    last_day = df_['datetime'].iloc[-1]
    print(f"Entrada para extracción de características cíclicas:\n{df_[['datetime', 'PM2.5', 'PM10', 'NOx', 'O3', 'CO']]}")

    print(200*'-')

    df_ = extract_cyclic_features(df_[['datetime', 'PM2.5', 'PM10', 'NOx', 'O3', 'CO']])

    print("Normalizando características ambientales:")
    print(df_.iloc[:, :5].head())
    array_features = scaler_features.transform(df_.iloc[:, :5])
    print(200*'-')
    print("Normalizando características cíclicas:")
    print(df_.iloc[:, 5:].head())
    array_features_local = scaler_features_local.transform(df_.iloc[:, 5:])
    print(200*'-')
    array_features_full = np.concatenate((array_features, array_features_local), axis=1)
    print(f"Entrada al modelo autoencoder (shape: {array_features_full.shape}):")
    print(array_features_full)

    array_features_full_batch = np.expand_dims(array_features_full, axis=0)

    print("Realizando predicción con autoencoder...")
    prediction = regressor.predict(array_features_full_batch)

    next_day = last_day + pd.Timedelta(days=1)
    prediction_flat = prediction.flatten()
    print(200*'-')
    print(f"Predicción generada (entrada a extracción cíclica): {prediction_flat}")
    next_day_features = pd.DataFrame(
        [[next_day] + prediction_flat.tolist()],
        columns=['datetime', 'PM2.5', 'PM10', 'NOx', 'O3', 'CO']
    )
    print(200*'-')
    print(f"Extrayendo características cíclicas para PCA:\n{next_day_features}")
    next_day_features = extract_cyclic_features(next_day_features)
    print(f"Características cíclicas extraídas:\n{next_day_features}")
    print(200*'-')
    print(f"Entrada al modelo PCA (shape: {next_day_features.shape}):")
    next_day_features_pca = pca_loaded.transform(next_day_features)
    print(200*'-')
    print("Realizando predicción de estado con clasificador...")
    state = stk_loaded.predict(next_day_features_pca)

    print(f"Predicción de estado: {state}")

    print(200*'-')
    print("Revirtiendo escala de la predicción:")
    prediction_inverse = scaler_features.inverse_transform(prediction)
    prediction_inverse_flat = prediction_inverse.flatten()
    print(f"Predicción final desnormalizada: {prediction_inverse_flat}")
    print(200*'-')
    print("Agregando predicción al DataFrame...")
    new_row = pd.DataFrame(
        [[next_day] + [station] + prediction_inverse_flat.tolist() + [state]],
        columns=['datetime', 'station_id', 'PM2.5', 'PM10', 'NOx', 'O3', 'CO', 'state']
    )
    df = pd.concat([df, new_row], ignore_index=True)
    break

print("\nProceso completado. Últimas predicciones generadas:")

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import load_model
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.losses import MeanSquaredError
import joblib
from datetime import datetime



# Cargar el DataFrame con la estación 102
station = 102
df = df_imputed_full_denormalized[df_imputed_full_denormalized['station_id'] == station]

# Cargar modelos entrenados
regressor = tf.keras.models.load_model(
    f'/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Regresiones_2/{station}/best_autoencoder.h5',
    custom_objects={'mse': MeanSquaredError}
)

pca_loaded = joblib.load(f'/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Clasificadores_2/{station}/pca_model.pkl')
stk_loaded = joblib.load(f'/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Clasificadores_2/{station}/stk_model.pkl')
scaler_features = joblib.load(f'/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Normalized_station/scaler_station_{station}.pkl')
scaler_features_local =  joblib.load(f'/content/drive/MyDrive/SAMSUNG_IA_2/Modelos_Normalized_station/scaler_features_local.pkl')


# Convertir cadena a datetime
start_date = datetime.strptime("2022-01-01", "%Y-%m-%d")
end_date = datetime.strptime("2022-01-31", "%Y-%m-%d")

# Iterar sobre cada día de enero 2022
for _ in range((end_date - start_date).days + 1):

    # Tomar los últimos 7 días
    df_ = df[-7:].copy()
    last_day = df_['datetime'].iloc[-1]

    # Extraer características cíclicas
    df_ = extract_cyclic_features(df_[['datetime', 'PM2.5', 'PM10', 'NOx', 'O3', 'CO']])

    array_features = scaler_features.transform(df_.iloc[:, :5]) # columnas ['PM2.5', 'PM10', 'NOx', 'O3', 'CO']
    array_features_local = scaler_features_local.transform(df_.iloc[:, 5:]) # columnas  sin_weekday cos_weekday  sin_weekofyear  cos_weekofyear  sin_month  cos_month day  dayofyear

    # Concatenar la normalización de caracteristicas
    array_features_full = np.concatenate((array_features, array_features_local), axis=1)

    # Aumentamos la dimensionalidad
    array_features_full_batch = np.expand_dims(array_features_full, axis=0)  # Aumentamos la dimensionalidad (1, 7, 13)

    # Hacer la predicción con el modelo de regresión
    prediction = regressor.predict(array_features_full_batch)


    # Calcular la siguiente fecha
    next_day = last_day + pd.Timedelta(days=1)

    # Asegurarse de que la predicción sea un array unidimensional
    prediction_flat = prediction.flatten()

    # Crear un DataFrame con la nueva fecha y las predicciones
    next_day_features = pd.DataFrame(
        [[next_day] + prediction_flat.tolist()],  # Lista con datetime + valores predichos
        columns=['datetime', 'PM2.5', 'PM10', 'NOx', 'O3', 'CO']  # Columnas correctas
    )

    # Extraer características cíclicas para la nueva fecha
    next_day_features = extract_cyclic_features(next_day_features)

    # Aplicar PCA
    next_day_features_pca = pca_loaded.transform(next_day_features)

    # Predecir el estado con el modelo de clasificación
    state = stk_loaded.predict(next_day_features_pca)


    # Revertir la prediccion
    prediction_inverse = scaler_features.inverse_transform(prediction)
    prediction_inverse_flat = prediction_inverse.flatten()

    # Agregar la nueva predicción al DataFrame (para la siguiente iteración)
    new_row = pd.DataFrame(
        [[next_day] +[station]+ prediction_inverse_flat.tolist() + [state]],
        columns=['datetime','station_id', 'PM2.5', 'PM10', 'NOx', 'O3', 'CO', 'state']
    )

    df = pd.concat([df, new_row], ignore_index=True)

# Mostrar las predicciones finales
print(df.tail(32))

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import load_model
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler, FunctionTransformer
from tensorflow.keras.losses import MeanSquaredError


# Seleccionar solo el último mes
last_month = df['datetime'].max() - pd.DateOffset(months=2)  # El primer día del mes anterior
df_last_month = df[df['datetime'] > last_month]

# Normalizar las columnas numéricas (a partir de la columna 2, excepto state)
df_last_month_ = df_last_month.copy()
df_last_month_.iloc[:, 2:-1] = MinMaxScaler().fit_transform(df_last_month.iloc[:, 2:-1])
numerical_cols = ['PM2.5', 'PM10', 'NOx', 'O3', 'CO']
plot_time_series(df_last_month_, numerical_cols, station)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from sklearn.preprocessing import MinMaxScaler

# Agrupar por 'station_id' y mes, y calcular el promedio
df_grouped = df.groupby([pd.Grouper(key='datetime', freq='ME'), 'station_id'])[numerical_cols].mean().reset_index()

# Normalizar los datos usando MinMaxScaler
scaler = MinMaxScaler()
df_grouped[numerical_cols] = scaler.fit_transform(df_grouped[numerical_cols])

# Obtener las estaciones únicas
stations = df_grouped['station_id'].unique()

# Crear un gráfico por cada estación
for station in stations:
    # Filtrar los datos para la estación actual
    df_station = df_grouped[df_grouped['station_id'] == station]

    # Graficar la evolución temporal de cada variable para esta estación
    plt.figure(figsize=(14, 8))

    for col in numerical_cols:
        plt.plot(df_station['datetime'], df_station[col], label=col, linestyle='-')

    # Configurar el rango del eje X
    plt.xlim(pd.Timestamp('2010-01-01'), pd.Timestamp('2021-12-31'))

    # Configurar las etiquetas del eje X para que muestren año por año
    ax = plt.gca()
    ax.xaxis.set_major_locator(mdates.YearLocator(1))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))

    # Rotar las etiquetas del eje X para mejor legibilidad
    plt.xticks(rotation=45)

    # Añadir título y etiquetas
    plt.title(f"Evolución Temporal de Contaminantes y Variables Meteorológicas - Estación {station} PREDICCIÓN")
    plt.ylabel("Valor Normalizado (0 a 1)")
    plt.xlabel("Fecha")

    # Añadir leyenda
    plt.legend(title="Variable", bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
    plt.tight_layout()

    # Mostrar el gráfico
    plt.show()
    break